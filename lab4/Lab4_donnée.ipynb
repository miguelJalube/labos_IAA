{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43819f57-439c-4893-988e-2e89b187f258",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LAB 4: Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cb856-a2b7-47e9-8357-e56621808bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b5700-8460-41de-b1cc-93b16329f553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff03bd6-8c2d-4069-95f7-9c3a8fd7e65e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47864d32-e98d-44f9-8475-6c8576448fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e07fe3-f152-4aed-93c2-91db99d7300a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306cb41-2a1d-4491-9a6b-0ae0853cb586",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5, bias=False)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120, bias=False)\n",
    "        self.fc2 = nn.Linear(120, 84, bias=False)\n",
    "        self.fc3 = nn.Linear(84, 10, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cfb2f-42c9-4b75-9576-4256afcbeee4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6dd1d-42b0-4cd2-9282-ecbbb6a46880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(model: nn.Module, dataloader: DataLoader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "def test(model: nn.Module, dataloader: DataLoader, max_samples=None) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    n_inferences = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            print(images.size())\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            if max_samples:\n",
    "                n_inferences += images.shape[0]\n",
    "                if n_inferences > max_samples:\n",
    "                    break\n",
    "    \n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921b04b-197c-4a12-8f62-205e98b5766c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(net, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a8a88-d88b-4b90-812c-f831f4138721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = test(net, testloader)\n",
    "print('Accuracy of the network on the test images: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb1a47-7f6c-42f4-9f8d-94d353a18d62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization of weights distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d86d23-a105-4104-93a8-995b48a1c40f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE to plot distributions of weights\n",
    "\n",
    "# You can get a flattened vector of the weights of fc1 like this:\n",
    "#   fc1_weights = net.fc1.weight.data.cpu().view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b9e7f-ee74-400b-b0ec-1d348e90f796",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Post training quantization: Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63852b-80d2-469a-ac00-6330526b8e4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# A convenience function which we use to copy CNNs\n",
    "def copy_model(model: nn.Module) -> nn.Module:\n",
    "    result = deepcopy(model)\n",
    "    if hasattr(model, 'input_activations'):\n",
    "        result.input_activations = deepcopy(model.input_activations)\n",
    "\n",
    "    for result_layer, original_layer in zip(result.children(), model.children()):\n",
    "        if isinstance(result_layer, nn.Conv2d) or isinstance(result_layer, nn.Linear):\n",
    "            if hasattr(original_layer.weight, 'scale'):\n",
    "                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n",
    "            if hasattr(original_layer, 'activations'):\n",
    "                result_layer.activations = deepcopy(original_layer.activations)\n",
    "            if hasattr(original_layer, 'output_scale'):\n",
    "                result_layer.output_scale = deepcopy(original_layer.output_scale)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6bd80-4d54-472d-ae7e-d5e25928a5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def quantized_weights(weights: torch.Tensor, target_range: float = 127.0) -> Tuple[torch.Tensor, float]:\n",
    "    '''\n",
    "    Quantize the weights so that all values are integers between -128 and 127.\n",
    "    You may want to use the total range, 3-sigma range, or some other range when\n",
    "    deciding just what factors to scale the float32 values by.\n",
    "\n",
    "    Parameters:\n",
    "    weights (Tensor): The unquantized weights\n",
    "    target_range (float): The desired range for the quantized values. Default is 127.0.\n",
    "\n",
    "    Returns:\n",
    "    (Tensor, float): A tuple with the following elements:\n",
    "                        * The weights in quantized form, where every value is an integer between -128 and 127.\n",
    "                          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n",
    "                        * The scaling factor that your weights were multiplied by.\n",
    "                          This value does not need to be an 8-bit integer.\n",
    "    '''\n",
    "    \n",
    "    #ADD YOUR CODE HERE\n",
    "    \n",
    "return weights, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4466f089-0775-4495-bc2c-0f626ba2250e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_q2 = copy_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b54c30-a593-4fcc-8de8-1fa6568822e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantize_layer_weights(model: nn.Module):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            q_layer_data, scale = quantized_weights(layer.weight.data, 3*net.fc3.weight.data.cpu().view(-1).std())\n",
    "            q_layer_data = q_layer_data.to(device)\n",
    "\n",
    "            layer.weight.data = q_layer_data\n",
    "            layer.weight.scale = scale\n",
    "\n",
    "            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n",
    "                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n",
    "            if (q_layer_data != q_layer_data.round()).any():\n",
    "                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n",
    "\n",
    "quantize_layer_weights(net_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fb2ee7-7f50-4226-b93a-aa9d3f312d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = test(net_q2, testloader)\n",
    "print('Accuracy of the network after quantizing all weights: {}%'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a0ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print the size of the model\n",
    "## ADD YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d69f32-1e68-4ccd-a67e-f2ca25e7b4df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Quantization activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf09a6e-e24d-41a3-b3f6-f1a6c831e77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def register_activation_profiling_hooks(model: Net):\n",
    "    model.input_activations = np.empty(0)\n",
    "    model.conv1.activations = np.empty(0)\n",
    "    model.conv2.activations = np.empty(0)\n",
    "    model.fc1.activations = np.empty(0)\n",
    "    model.fc2.activations = np.empty(0)\n",
    "    model.fc3.activations = np.empty(0)\n",
    "\n",
    "    model.profile_activations = True\n",
    "\n",
    "    def conv1_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.input_activations = np.append(model.input_activations, x[0].cpu().view(-1))\n",
    "    model.conv1.register_forward_hook(conv1_activations_hook)\n",
    "\n",
    "    def conv2_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.conv1.activations = np.append(model.conv1.activations, x[0].cpu().view(-1))\n",
    "    model.conv2.register_forward_hook(conv2_activations_hook)\n",
    "\n",
    "    def fc1_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.conv2.activations = np.append(model.conv2.activations, x[0].cpu().view(-1))\n",
    "    model.fc1.register_forward_hook(fc1_activations_hook)\n",
    "\n",
    "    def fc2_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.fc1.activations = np.append(model.fc1.activations, x[0].cpu().view(-1))\n",
    "    model.fc2.register_forward_hook(fc2_activations_hook)\n",
    "\n",
    "    def fc3_activations_hook(layer, x, y):\n",
    "        if model.profile_activations:\n",
    "            model.fc2.activations = np.append(model.fc2.activations, x[0].cpu().view(-1))\n",
    "            model.fc3.activations = np.append(model.fc3.activations, y[0].cpu().view(-1))\n",
    "    model.fc3.register_forward_hook(fc3_activations_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af1254-c4d3-4015-ba4b-83b333776de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net_q3 = copy_model(net)\n",
    "register_activation_profiling_hooks(net_q3)\n",
    "\n",
    "# Run through the training dataset again while profiling the input and output activations this time\n",
    "# We don't actually have to perform gradient descent for this, so we can use the \"test\" function\n",
    "test(net_q3, trainloader, max_samples=400)\n",
    "net_q3.profile_activations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf28b1-5deb-4dce-a118-a59154fec6a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualize activation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc85f64-c27c-41f6-8449-80f4120f84ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADD YOUR CODE HERE: to plot activation distribution\n",
    "# You can access the activation matrice like this:\n",
    "# net_q3.input_activations\n",
    "# net_q3.conv1_activations\n",
    "# ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07aebf0-0287-4ef0-b86a-b4a24579d23b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Quantize activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e1d4d-206d-424c-8d47-9961410d168f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class NetQuantized(nn.Module):\n",
    "    def __init__(self, net_with_weights_quantized: nn.Module):\n",
    "        super(NetQuantized, self).__init__()\n",
    "        \n",
    "        net_init = copy_model(net_with_weights_quantized)\n",
    "\n",
    "        self.conv1 = net_init.conv1\n",
    "        self.pool = net_init.pool\n",
    "        self.conv2 = net_init.conv2\n",
    "        self.fc1 = net_init.fc1\n",
    "        self.fc2 = net_init.fc2\n",
    "        self.fc3 = net_init.fc3\n",
    "\n",
    "        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n",
    "            def pre_hook(l, x):\n",
    "                x = x[0]\n",
    "                if (x < -128).any() or (x > 127).any():\n",
    "                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n",
    "                if (x != x.round()).any():\n",
    "                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n",
    "\n",
    "            layer.register_forward_pre_hook(pre_hook)\n",
    "\n",
    "        # Calculate the scaling factor for the initial input to the CNN\n",
    "        self.input_activations = net_with_weights_quantized.input_activations\n",
    "        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n",
    "\n",
    "        # Calculate the output scaling factors for all the layers of the CNN\n",
    "        preceding_layer_scales = []\n",
    "        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n",
    "            layer.output_scale = NetQuantized.quantize_activations(layer.activations, layer.weight.scale, self.input_scale, preceding_layer_scales)\n",
    "            preceding_layer_scales.append((layer.weight.scale, layer.output_scale))\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_initial_input(initial_input: np.ndarray) -> float:\n",
    "        '''\n",
    "        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n",
    "\n",
    "        Parameters:\n",
    "        initial_input (ndarray): The values of all the pixels which were part of the input image during training\n",
    "\n",
    "        Returns:\n",
    "        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n",
    "               This value does not need to be an 8-bit integer.\n",
    "        '''\n",
    "\n",
    "        # ADD your code here\n",
    "\n",
    "        return scale_factor\n",
    "\n",
    "    @staticmethod\n",
    "    def quantize_activations(activations: np.ndarray, s_w: float, s_initial_input: float, ns: List[Tuple[float, float]]) -> float:\n",
    "        '''\n",
    "        Calculate a scaling factor to multiply the output of a layer by.\n",
    "\n",
    "        Parameters:\n",
    "        activations (ndarray): The values of all the pixels which have been output by this layer during training\n",
    "        s_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n",
    "        s_initial_input (float): The scale by which the initial input to the neural network was multiplied\n",
    "        ns ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n",
    "\n",
    "        Returns:\n",
    "        float: A scaling factor that the layer output should be multiplied by before being fed into the next layer.\n",
    "               This value does not need to be an 8-bit integer.\n",
    "        '''\n",
    "        # ADD YOUR CODE HERE\n",
    "       \n",
    "        return scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # You can access the output activation scales like this:\n",
    "        #   fc1_output_scale = self.fc1.output_scale\n",
    "        # You don't need to quantize Relu outputs \n",
    "\n",
    "        # ADD YOUR CODE HERE (Replace the ...)\n",
    "        # ...\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = torch.clamp(x, min=-128, max=127)\n",
    "        # ...\n",
    "        x = self.pool(F.relu(x))\n",
    "        x = torch.clamp(x, min=-128, max=127)\n",
    "        # ...\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # ...\n",
    "        x = F.relu(x)\n",
    "        # ...\n",
    "        x = F.relu(x)\n",
    "        # ...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbe535-797c-402d-a0a3-11e7d6c3408c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the information from net_q2 and net_q3 together\n",
    "\n",
    "net_init = copy_model(net_q2)\n",
    "net_init.input_activations = deepcopy(net_q3.input_activations)\n",
    "for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n",
    "    if isinstance(layer_init, nn.Conv2d) or isinstance(layer_init, nn.Linear):\n",
    "        layer_init.activations = deepcopy(layer_q3.activations)\n",
    "\n",
    "net_quantized = NetQuantized(net_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722eb2d-bb2e-44d6-bbd0-b6bbd0db9329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score = test(net_quantized, testloader)\n",
    "print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
